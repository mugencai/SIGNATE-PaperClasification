{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文本截断句首句尾 OK!\n",
    "#使用0填充缺失值\n",
    "\n",
    "\n",
    "#预训练模型换pubmedbert试试 OK！\n",
    "#使用结合了abstract和text的pubmedbert ok！\n",
    "#文本使用title+abstract OK!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#阈值参与到了模型训练和预测中                                  #####\n",
    "#那么它是如何发挥作用的，为何在预测步骤调整阈值反而会让LB上升？####\n",
    "#在最前面就将阈值调成0.02试试\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#输出函数由Sigmoid换成Softmax试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers as T\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter\n",
    "SEED = 42\n",
    "\n",
    "BORDER = 0.000023\n",
    "\n",
    "K_FOLD = 2\n",
    "\n",
    "MAX_LEN = 512\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "LR = 5e-5\n",
    "\n",
    "INFERENCE_DATALOADER_BATCHSIZE = 28\n",
    "\n",
    "TRAIN_DATALOADER_BATCHSIZE = 28\n",
    "\n",
    "EPOCH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./\"\n",
    "OUTPUT_DIR = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n",
    "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
    "\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the seed value all over to make the result reproducible\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = SEED\n",
    "seed_torch(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\dai\\data\\paper_classification\\train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\dai\\data\\paper_classification\\test.csv\")\n",
    "\n",
    "\n",
    "#拿sample当submission的样板\n",
    "sub = pd.read_csv(r\"C:\\Users\\dai\\data\\paper_classification\\sample_submit.csv\", header=None)\n",
    "sub.columns = [\"id\", \"judgement\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#填充nan\n",
    "train['abstract'].fillna(value=\"0\", inplace = True)\n",
    "test['abstract'].fillna(value=\"0\", inplace = True)\n",
    "\n",
    "\n",
    "#融合title + abstract\n",
    "train[\"text\"] = train[\"title\"] + train[\"abstract\"]\n",
    "train = train.drop(['abstract', 'title'], axis=1)\n",
    "test[\"text\"] = test[\"title\"] + test[\"abstract\"]\n",
    "test = test.drop(['abstract', 'title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27145</td>\n",
       "      <td>Estimating the potential effects of COVID-19 p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27146</td>\n",
       "      <td>Leukoerythroblastic reaction in a patient with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27147</td>\n",
       "      <td>[15O]-water PET and intraoperative brain mappi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27148</td>\n",
       "      <td>Adaptive image segmentation for robust measure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27149</td>\n",
       "      <td>Comparison of Epidemiological Variations in CO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40829</th>\n",
       "      <td>67974</td>\n",
       "      <td>Knowledge, Attitude, and Practices of Healthca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40830</th>\n",
       "      <td>67975</td>\n",
       "      <td>Safety and Efficacy of Anti-Il6-Receptor Tocil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40831</th>\n",
       "      <td>67976</td>\n",
       "      <td>Functional imaging of head and neck tumors usi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40832</th>\n",
       "      <td>67977</td>\n",
       "      <td>Effectiveness of 3D virtual imaging0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40833</th>\n",
       "      <td>67978</td>\n",
       "      <td>A prospective evaluation of thallium-201 singl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40834 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text\n",
       "0      27145  Estimating the potential effects of COVID-19 p...\n",
       "1      27146  Leukoerythroblastic reaction in a patient with...\n",
       "2      27147  [15O]-water PET and intraoperative brain mappi...\n",
       "3      27148  Adaptive image segmentation for robust measure...\n",
       "4      27149  Comparison of Epidemiological Variations in CO...\n",
       "...      ...                                                ...\n",
       "40829  67974  Knowledge, Attitude, and Practices of Healthca...\n",
       "40830  67975  Safety and Efficacy of Anti-Il6-Receptor Tocil...\n",
       "40831  67976  Functional imaging of head and neck tumors usi...\n",
       "40832  67977               Effectiveness of 3D virtual imaging0\n",
       "40833  67978  A prospective evaluation of thallium-201 singl...\n",
       "\n",
       "[40834 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023282372444280715\n"
     ]
    }
   ],
   "source": [
    "# この値を境に、モデルの出力を 0 と 1 にします。\n",
    "border = len(train[train[\"judgement\"] == 1]) / len(train[\"judgement\"])\n",
    "print(border)\n",
    "\n",
    "border = BORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(train):\n",
    "    #使用sklearn的StratifiedKFold类（k折，随机），\n",
    "    #详情看https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "    Fold = StratifiedKFold(n_splits=K_FOLD, shuffle=True, random_state=seed)\n",
    "    #使用split方法给train编号，用编号划分训练集和验证集\n",
    "    #n代表enumerate带来的每个(train_index, val_index)的index，\n",
    "    #for循环n_splits次，n也就从0到4\n",
    "    for n, (train_index, val_index) in enumerate(Fold.split(train, train[\"judgement\"])):\n",
    "        #在train中新增\"fold\"列，用以记录\"val_index\"行属于第几折\n",
    "        train.loc[val_index, \"fold\"] = int(n)\n",
    "    train[\"fold\"] = train[\"fold\"].astype(np.uint8)\n",
    "    \n",
    "    #返回的训练集中列有\"id\",\"text\",\"submission\",\"fold\"\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(test):\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "train = get_train_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>judgement</th>\n",
       "      <th>text</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>One-year age changes in MRI brain volumes in o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Supportive CSF biomarker evidence to enhance t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Occurrence of basal ganglia germ cell tumors w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>New developments in diagnosis and therapy of C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Prolonged shedding of SARS-CoV-2 in an elderly...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27140</th>\n",
       "      <td>27140</td>\n",
       "      <td>0</td>\n",
       "      <td>The amyloidogenic pathway of amyloid precursor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27141</th>\n",
       "      <td>27141</td>\n",
       "      <td>0</td>\n",
       "      <td>Technologic developments in radiotherapy and s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27142</th>\n",
       "      <td>27142</td>\n",
       "      <td>0</td>\n",
       "      <td>Novel screening cascade identifies MKK4 as key...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27143</th>\n",
       "      <td>27143</td>\n",
       "      <td>0</td>\n",
       "      <td>Visualization of the gall bladder on F-18 FDOP...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27144</th>\n",
       "      <td>27144</td>\n",
       "      <td>0</td>\n",
       "      <td>Multidetector CT findings and differential dia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27145 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  judgement                                               text  \\\n",
       "0          0          0  One-year age changes in MRI brain volumes in o...   \n",
       "1          1          0  Supportive CSF biomarker evidence to enhance t...   \n",
       "2          2          0  Occurrence of basal ganglia germ cell tumors w...   \n",
       "3          3          0  New developments in diagnosis and therapy of C...   \n",
       "4          4          0  Prolonged shedding of SARS-CoV-2 in an elderly...   \n",
       "...      ...        ...                                                ...   \n",
       "27140  27140          0  The amyloidogenic pathway of amyloid precursor...   \n",
       "27141  27141          0  Technologic developments in radiotherapy and s...   \n",
       "27142  27142          0  Novel screening cascade identifies MKK4 as key...   \n",
       "27143  27143          0  Visualization of the gall bladder on F-18 FDOP...   \n",
       "27144  27144          0  Multidetector CT findings and differential dia...   \n",
       "\n",
       "       fold  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "27140     1  \n",
       "27141     0  \n",
       "27142     0  \n",
       "27143     1  \n",
       "27144     1  \n",
       "\n",
       "[27145 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27145.000000\n",
       "mean      1353.749051\n",
       "std        785.755907\n",
       "min         17.000000\n",
       "25%        853.000000\n",
       "50%       1464.000000\n",
       "75%       1856.000000\n",
       "max       9690.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"text\"].map(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    40834.000000\n",
       "mean      1356.380320\n",
       "std        788.957966\n",
       "min         17.000000\n",
       "25%        860.000000\n",
       "50%       1461.000000\n",
       "75%       1853.000000\n",
       "max      12934.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"text\"].map(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One-year age changes in MRI brain volumes in o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Supportive CSF biomarker evidence to enhance t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Occurrence of basal ganglia germ cell tumors w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New developments in diagnosis and therapy of C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prolonged shedding of SARS-CoV-2 in an elderly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67974</th>\n",
       "      <td>Knowledge, Attitude, and Practices of Healthca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67975</th>\n",
       "      <td>Safety and Efficacy of Anti-Il6-Receptor Tocil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67976</th>\n",
       "      <td>Functional imaging of head and neck tumors usi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67977</th>\n",
       "      <td>Effectiveness of 3D virtual imaging0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67978</th>\n",
       "      <td>A prospective evaluation of thallium-201 singl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67979 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      One-year age changes in MRI brain volumes in o...\n",
       "1      Supportive CSF biomarker evidence to enhance t...\n",
       "2      Occurrence of basal ganglia germ cell tumors w...\n",
       "3      New developments in diagnosis and therapy of C...\n",
       "4      Prolonged shedding of SARS-CoV-2 in an elderly...\n",
       "...                                                  ...\n",
       "67974  Knowledge, Attitude, and Practices of Healthca...\n",
       "67975  Safety and Efficacy of Anti-Il6-Receptor Tocil...\n",
       "67976  Functional imaging of head and neck tumors usi...\n",
       "67977               Effectiveness of 3D virtual imaging0\n",
       "67978  A prospective evaluation of thallium-201 singl...\n",
       "\n",
       "[67979 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ = pd.concat((train[\"text\"],test[\"text\"]),axis= 0)\n",
    "text_ = pd.DataFrame(text_)\n",
    "\n",
    "text_ = text_.reset_index(drop=True)\n",
    "text_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frontotemporal dementia can be distinguished from Alzheimer's disease and subcortical white matter dementia by an anterior-to-posterior rCBF-SPET ratio.Sixteen patients with frontotemporal dementia (FTD)  27 with early-onset Alzheimer's disease  25 with late-onset Alzheimer's disease  19 with subcortical white matter dementia (SWD) and 28 normal controls underwent semiquantitative regional cerebral blood flow measurement (rCBF) using single-photon emission tomography (SPET; (99m)Tc-HMPAO) and either computerized tomography (CT) or magnetic resonance imaging (MRI) of the brain. An anterior-to-posterior rCBF-SPET ratio (mesial superior frontal gyrus/medial temporal lobes) was calculated  which significantly separated the FTD group from the other dementia groups and controls with a sensitivity of 87.5% and a specificity of at least 78.6%. CT/MRI was found to be helpful in the differential diagnosis between FTD and SWD. In FTD patients  the mesial superior frontal gyrus  near the polus frontalis  was found to be the region with the most reduced rCBF values.\n",
      "\n",
      "\n",
      "Frontotemporal dementia can be distinguished from Alzheimer's di\n",
      "\n",
      "\n",
      "Frontotemporal dementia can be distinguished from Alzheimer's dis  was found to be the region with the most reduced rCBF values.\n"
     ]
    }
   ],
   "source": [
    "print(text_[\"text\"][56])\n",
    "print(\"\\n\")\n",
    "print(text_[\"text\"][56][:64])\n",
    "print(\"\\n\")\n",
    "text_56 = text_[\"text\"][56][:64] + text_[\"text\"][56][-64:] \n",
    "print(text_56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#句子首尾截断\n",
    "# maxlen = MAX_LEN\n",
    "# for index in range(len(text_[\"text\"])):    \n",
    "#     if len(text_[\"text\"][index]) > maxlen:\n",
    "#         maxlen_ = maxlen//2\n",
    "#         text_[\"text\"][index] = text_[\"text\"][index][:maxlen_] + text_[\"text\"][index][-maxlen_:]\n",
    "        \n",
    "# text_[\"text\"].map(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文本适用截取完的版本\n",
    "# train[\"text\"] = text_[\"text\"][:len(train)].values\n",
    "# test[\"text\"] = text_[\"text\"][len(train):].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27145.000000\n",
       "mean      1353.749051\n",
       "std        785.755907\n",
       "min         17.000000\n",
       "25%        853.000000\n",
       "50%       1464.000000\n",
       "75%       1856.000000\n",
       "max       9690.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"text\"].map(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    40834.000000\n",
       "mean      1356.380320\n",
       "std        788.957966\n",
       "min         17.000000\n",
       "25%        860.000000\n",
       "50%       1461.000000\n",
       "75%       1853.000000\n",
       "max      12934.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"text\"].map(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, df, model_name, include_labels=True):\n",
    "        #引入BertTokenizer.from_pretrained类\n",
    "        #详情参照https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.batch_encode_plus\n",
    "        tokenizer = T.BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        self.df = df\n",
    "        self.include_labels = include_labels\n",
    "\n",
    "        self.text = df[\"text\"].tolist()\n",
    "        #使用batch_encode_plus对多个句子编码\n",
    "        #编码包括：\n",
    "        #Tokenize sentences\n",
    "        #Prepend the [CLS] token to the start \n",
    "        #Append the [SEP] token to the end\n",
    "        #Map tokens to their IDs\n",
    "        #Make IDs same max_length  \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',            \n",
    "            max_length = MAX_LENGTH,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "        if self.include_labels:\n",
    "            self.labels = df[\"judgement\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #为啥这里要加[idx]?\n",
    "        #表示标记id表和注意力掩码的索引\n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][idx])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][idx])\n",
    "\n",
    "        if self.include_labels:\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "            return input_ids, attention_mask, label\n",
    "\n",
    "        return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = T.BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        out = self.softmax(out.logits).squeeze()\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#辅助函数\n",
    "\n",
    "class AverageMeter(object):\n",
    "    #计算平均值用\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    #将秒转化为分\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    #持续时间\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    start = end = time.time()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        y_preds = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(y_preds, labels.long())\n",
    "\n",
    "        # record loss\n",
    "        #使用AverageMeter()更新并记录误差\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0 or step == (len(train_loader) - 1):\n",
    "            print(\n",
    "                f\"Epoch: [{epoch + 1}][{step}/{len(train_loader)}] \"\n",
    "                f\"Elapsed {timeSince(start, float(step + 1) / len(train_loader)):s} \"\n",
    "                f\"Loss: {losses.avg:.4f} \"\n",
    "            )\n",
    "    \n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    start = end = time.time()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(valid_loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(y_preds, labels.long())\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # record score\n",
    "        preds.append(y_preds.to(\"cpu\").numpy())\n",
    "\n",
    "        if step % 100 == 0 or step == (len(valid_loader) - 1):\n",
    "            print(\n",
    "                f\"EVAL: [{step}/{len(valid_loader)}] \"\n",
    "                f\"Elapsed {timeSince(start, float(step + 1) / len(valid_loader)):s} \"\n",
    "                f\"Loss: {losses.avg:.4f} \"\n",
    "            )\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    \n",
    "#     predictions = list(predictions)\n",
    "#     print(predictions)\n",
    "#     print(predictions.shape())\n",
    "    return losses.avg, predictions\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "  \n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
    "# model = AutoModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataloader_batchsize = INFERENCE_DATALOADER_BATCHSIZE\n",
    "\n",
    "#对测试集进行推理，获得预测结果\n",
    "def inference():\n",
    "    predictions = []\n",
    "\n",
    "    test_dataset = BaseDataset(test, \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\", include_labels=False)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=inference_dataloader_batchsize,\n",
    "        shuffle=False, num_workers=0, pin_memory=True,\n",
    "    )\n",
    "\n",
    "    for fold in range(K_FOLD):\n",
    "        LOGGER.info(f\"========== model: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext fold: {fold} inference ==========\")\n",
    "        model = BaseModel(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "        model.to(device)\n",
    "        model.load_state_dict(torch.load(OUTPUT_DIR + f\"PubMedBERT_base_uncased_fold{fold}_best.pth\")[\"model\"])\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        for i, (input_ids, attention_mask) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(input_ids, attention_mask)\n",
    "            preds.append(y_preds.to(\"cpu\").numpy())\n",
    "        preds = np.concatenate(preds)\n",
    "        predictions.append(preds)\n",
    "    predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloop_dataloader_batchsize = TRAIN_DATALOADER_BATCHSIZE\n",
    "\n",
    "#将经过get_train_data()函数处理的train放入到train_loop()中\n",
    "def train_loop(train, fold):\n",
    "\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # Data Loader\n",
    "    # ====================================================\n",
    "    #train[\"fold\"]不是第fold的行，其index记为trn_idx；\n",
    "    #反之其index记为val_index\n",
    "    trn_idx = train[train[\"fold\"] != fold].index\n",
    "    val_idx = train[train[\"fold\"] == fold].index\n",
    "    \n",
    "    #对训练集验证集重新reset索引\n",
    "    train_folds = train.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = train.loc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    #使用BasedataSet函数对数据集进行编码\n",
    "    train_dataset = BaseDataset(train_folds, \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "    valid_dataset = BaseDataset(valid_folds, \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=trainloop_dataloader_batchsize,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=trainloop_dataloader_batchsize,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # Model\n",
    "    # ====================================================\n",
    "    model = BaseModel(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = T.AdamW(model.parameters(),\n",
    "                        lr=LR,)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "    # ====================================================\n",
    "    # Loop\n",
    "    # ====================================================\n",
    "    best_score = -1\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        #train_fn函数返回训练集平均误差\n",
    "        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, device)\n",
    "\n",
    "        # eval\n",
    "        #valid_fn函数返回验证集平均误差和预测结果\n",
    "        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
    "        valid_labels = valid_folds[\"judgement\"].values\n",
    "#         print(valid_labels)\n",
    "\n",
    "        # scoring\n",
    "        pred_list = []\n",
    "        for id_x in range(len(preds)):\n",
    "#             print(preds[id_x])\n",
    "#             print(preds[id_x][1])\n",
    "            pred_list.append(preds[id_x][1])\n",
    "#         print(pred_list)\n",
    "        preds = np.array(pred_list)\n",
    "        \n",
    "        score = fbeta_score(valid_labels, np.where(preds < border, 0, 1), beta=7.0)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        LOGGER.info(\n",
    "            f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\"\n",
    "        )\n",
    "        LOGGER.info(f\"Epoch {epoch+1} - Score: {score}\")\n",
    "        \n",
    "        #如果验证集的得分比最佳得分好，就取代最佳得分，\n",
    "        #并将模型参数保存下来\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f\"Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model\")\n",
    "            torch.save(\n",
    "                {\"model\": model.state_dict(), \"preds\": preds}, OUTPUT_DIR + f\"PubMedBERT_base_uncased_fold{fold}_best.pth\"\n",
    "            )\n",
    "    \n",
    "    #读取最佳模型参数\n",
    "    check_point = torch.load(OUTPUT_DIR + f\"PubMedBERT_base_uncased_fold{fold}_best.pth\")\n",
    "\n",
    "    valid_folds[\"preds\"] = check_point[\"preds\"]\n",
    "\n",
    "    #返回包含最佳预测结果的验证集\n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算F7score得分\n",
    "def get_result(result_df):\n",
    "    preds = result_df[\"preds\"].values\n",
    "    labels = result_df[\"judgement\"].values\n",
    "    score = fbeta_score(labels, np.where(preds < border, 0, 1), beta=7.0)\n",
    "    LOGGER.info(f\"Score: {score:<.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[:8000]\n",
    "test = test[:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/143] Elapsed 0m 2s (remain 4m 53s) Loss: 0.6618 \n",
      "Epoch: [1][100/143] Elapsed 0m 53s (remain 0m 22s) Loss: 0.3403 \n",
      "Epoch: [1][142/143] Elapsed 1m 15s (remain 0m 0s) Loss: 0.3391 \n",
      "EVAL: [0/143] Elapsed 0m 0s (remain 0m 26s) Loss: 0.3490 \n",
      "EVAL: [100/143] Elapsed 0m 17s (remain 0m 7s) Loss: 0.3367 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.3391  avg_val_loss: 0.3369  time: 100s\n",
      "Epoch 1 - Score: 0.5471478463329453\n",
      "Epoch 1 - Save Best Score: 0.5471 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [142/143] Elapsed 0m 24s (remain 0m 0s) Loss: 0.3369 \n",
      "Epoch: [2][0/143] Elapsed 0m 0s (remain 1m 11s) Loss: 0.3133 \n",
      "Epoch: [2][100/143] Elapsed 0m 52s (remain 0m 21s) Loss: 0.3331 \n",
      "Epoch: [2][142/143] Elapsed 1m 14s (remain 0m 0s) Loss: 0.3343 \n",
      "EVAL: [0/143] Elapsed 0m 0s (remain 0m 26s) Loss: 0.3490 \n",
      "EVAL: [100/143] Elapsed 0m 17s (remain 0m 7s) Loss: 0.3366 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.3343  avg_val_loss: 0.3369  time: 99s\n",
      "Epoch 2 - Score: 0.5471478463329453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [142/143] Elapsed 0m 24s (remain 0m 0s) Loss: 0.3369 \n",
      "Epoch: [3][0/143] Elapsed 0m 0s (remain 1m 12s) Loss: 0.3490 \n",
      "Epoch: [3][100/143] Elapsed 0m 52s (remain 0m 22s) Loss: 0.3327 \n",
      "Epoch: [3][142/143] Elapsed 1m 14s (remain 0m 0s) Loss: 0.3343 \n",
      "EVAL: [0/143] Elapsed 0m 0s (remain 0m 26s) Loss: 0.3490 \n",
      "EVAL: [100/143] Elapsed 0m 18s (remain 0m 7s) Loss: 0.3366 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.3343  avg_val_loss: 0.3369  time: 100s\n",
      "Epoch 3 - Score: 0.5471478463329453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [142/143] Elapsed 0m 25s (remain 0m 0s) Loss: 0.3369 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 result ==========\n",
      "Score: 0.54715\n",
      "========== fold: 1 training ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/142] Elapsed 0m 0s (remain 1m 11s) Loss: 0.6660 \n",
      "Epoch: [1][100/142] Elapsed 0m 52s (remain 0m 21s) Loss: 0.3455 \n",
      "Epoch: [1][141/142] Elapsed 1m 14s (remain 0m 0s) Loss: 0.3425 \n",
      "EVAL: [0/144] Elapsed 0m 0s (remain 0m 26s) Loss: 0.3490 \n",
      "EVAL: [100/144] Elapsed 0m 17s (remain 0m 7s) Loss: 0.3356 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.3425  avg_val_loss: 0.3342  time: 99s\n",
      "Epoch 1 - Score: 0.5164781111657649\n",
      "Epoch 1 - Save Best Score: 0.5165 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [143/144] Elapsed 0m 24s (remain 0m 0s) Loss: 0.3342 \n",
      "Epoch: [2][0/142] Elapsed 0m 0s (remain 1m 21s) Loss: 0.3490 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-593e43327b27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moof_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK_FOLD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0m_oof_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0moof_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moof_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_oof_df\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mLOGGER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"========== fold: {fold} result ==========\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-dd5d9728a55d>\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(train, fold)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m#train_fn函数返回训练集平均误差\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mavg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;31m# eval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-f3a9d5bbc628>\u001b[0m in \u001b[0;36mtrain_fn\u001b[1;34m(train_loader, model, criterion, optimizer, epoch, device)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m#使用AverageMeter()更新并记录误差\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "oof_df = pd.DataFrame()\n",
    "for fold in range(K_FOLD):\n",
    "    _oof_df = train_loop(train, fold)\n",
    "    oof_df = pd.concat([oof_df, _oof_df])\n",
    "    LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "    #第k折的得分\n",
    "    get_result(_oof_df)\n",
    "\n",
    "# CV result\n",
    "#k折平均得分\n",
    "LOGGER.info(f\"========== CV ==========\")\n",
    "get_result(oof_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save OOF result\n",
    "# 存储包含最佳预测结果的验证集\n",
    "oof_df.to_csv(OUTPUT_DIR + \"oof_df_pubmed_16.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = ([[1,2],[1,2],[1,2],[1,2],[1,2]])\n",
    "# # preds = ([9.9897611e-01,1.0239244e-03],\n",
    "# #          [9.9908459e-01,9.1540796e-04],\n",
    "# #          [9.9740160e-01,2.5984009e-03],\n",
    "# #          [9.9910837e-01,8.9155516e-04],)\n",
    "\n",
    "\n",
    "# for id_x in range(len(preds)):\n",
    "#     preds[id_x] = preds[id_x][1]\n",
    "# preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "predictions = inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borders = []\n",
    "list = range(10,91,1)\n",
    "for x in list:\n",
    "    x = x/1000\n",
    "    borders.append(x)\n",
    "\n",
    "\n",
    "for border in borders:\n",
    "    preds = np.where(predictions < border, 0, 1)\n",
    "    # submission\n",
    "    sub[\"judgement\"] = preds\n",
    "    n = str(border)\n",
    "    sub.to_csv(OUTPUT_DIR + \"bert_copy16_border\"+n+\".csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
