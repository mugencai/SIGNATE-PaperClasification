{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练模型换pubmedber\n",
    "#加个k_fold OK!\n",
    "# 把border（Threshold）调低一点试试 \n",
    "#将nan填充值由“NotANumber”变为“0” OK!\n",
    "#将模型由BertForSequenceClassification换成XLNetForSequenceClassification\n",
    "#阈值参与到了模型训练和预测中                                  #####\n",
    "#那么它是如何发挥作用的，为何在预测步骤调整阈值反而会让LB上升？####\n",
    "#在最前面就将阈值调成0.02试试\n",
    "#使用结合了abstract和text的pubmedbert ok\n",
    "#弄清楚logger是怎么发挥作用的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers as T\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./\"\n",
    "OUTPUT_DIR = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n",
    "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
    "\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the seed value all over to make the result reproducible\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = 42\n",
    "seed_torch(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\dai\\data\\paper_classification\\train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\dai\\data\\paper_classification\\test.csv\")\n",
    "\n",
    "#拿sample当submission的样板\n",
    "sub = pd.read_csv(r\"C:\\Users\\dai\\data\\paper_classification\\sample_submit.csv\", header=None)\n",
    "sub.columns = [\"id\", \"judgement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#填充nan\n",
    "train['abstract'].fillna(value='0', inplace = True)\n",
    "test['abstract'].fillna(value='0', inplace = True)\n",
    "\n",
    "\n",
    "#融合title + abstract\n",
    "train[\"text\"] = train[\"title\"] + train[\"abstract\"]\n",
    "train = train.drop(['abstract', 'title'], axis=1)\n",
    "test[\"text\"] = test[\"title\"] + test[\"abstract\"]\n",
    "test = test.drop(['abstract', 'title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27145</td>\n",
       "      <td>Estimating the potential effects of COVID-19 p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27146</td>\n",
       "      <td>Leukoerythroblastic reaction in a patient with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27147</td>\n",
       "      <td>[15O]-water PET and intraoperative brain mappi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27148</td>\n",
       "      <td>Adaptive image segmentation for robust measure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27149</td>\n",
       "      <td>Comparison of Epidemiological Variations in CO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40829</th>\n",
       "      <td>67974</td>\n",
       "      <td>Knowledge, Attitude, and Practices of Healthca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40830</th>\n",
       "      <td>67975</td>\n",
       "      <td>Safety and Efficacy of Anti-Il6-Receptor Tocil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40831</th>\n",
       "      <td>67976</td>\n",
       "      <td>Functional imaging of head and neck tumors usi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40832</th>\n",
       "      <td>67977</td>\n",
       "      <td>Effectiveness of 3D virtual imaging0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40833</th>\n",
       "      <td>67978</td>\n",
       "      <td>A prospective evaluation of thallium-201 singl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40834 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text\n",
       "0      27145  Estimating the potential effects of COVID-19 p...\n",
       "1      27146  Leukoerythroblastic reaction in a patient with...\n",
       "2      27147  [15O]-water PET and intraoperative brain mappi...\n",
       "3      27148  Adaptive image segmentation for robust measure...\n",
       "4      27149  Comparison of Epidemiological Variations in CO...\n",
       "...      ...                                                ...\n",
       "40829  67974  Knowledge, Attitude, and Practices of Healthca...\n",
       "40830  67975  Safety and Efficacy of Anti-Il6-Receptor Tocil...\n",
       "40831  67976  Functional imaging of head and neck tumors usi...\n",
       "40832  67977               Effectiveness of 3D virtual imaging0\n",
       "40833  67978  A prospective evaluation of thallium-201 singl...\n",
       "\n",
       "[40834 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023282372444280715\n"
     ]
    }
   ],
   "source": [
    "# この値を境に、モデルの出力を 0 と 1 にします。\n",
    "border = len(train[train[\"judgement\"] == 1]) / len(train[\"judgement\"])\n",
    "print(border)\n",
    "\n",
    "border = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(train):\n",
    "\n",
    "    #使用sklearn的StratifiedKFold类（k折，随机），\n",
    "    #详情看https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "    Fold = StratifiedKFold(n_splits=k_fold, shuffle=True, random_state=seed)\n",
    "    #使用split方法给train编号，用编号划分训练集和验证集\n",
    "    #n代表enumerate带来的每个(train_index, val_index)的index，\n",
    "    #for循环n_splits次，n也就从0到4\n",
    "    for n, (train_index, val_index) in enumerate(Fold.split(train, train[\"judgement\"])):\n",
    "        #在train中新增\"fold\"列，用以记录\"val_index\"行属于第几折\n",
    "        train.loc[val_index, \"fold\"] = int(n)\n",
    "    train[\"fold\"] = train[\"fold\"].astype(np.uint8)\n",
    "    \n",
    "    #返回的训练集中列有\"id\",\"text\",\"submission\",\"fold\"\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(test):\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_train_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>judgement</th>\n",
       "      <th>text</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>One-year age changes in MRI brain volumes in o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Supportive CSF biomarker evidence to enhance t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Occurrence of basal ganglia germ cell tumors w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>New developments in diagnosis and therapy of C...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Prolonged shedding of SARS-CoV-2 in an elderly...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27140</th>\n",
       "      <td>27140</td>\n",
       "      <td>0</td>\n",
       "      <td>The amyloidogenic pathway of amyloid precursor...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27141</th>\n",
       "      <td>27141</td>\n",
       "      <td>0</td>\n",
       "      <td>Technologic developments in radiotherapy and s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27142</th>\n",
       "      <td>27142</td>\n",
       "      <td>0</td>\n",
       "      <td>Novel screening cascade identifies MKK4 as key...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27143</th>\n",
       "      <td>27143</td>\n",
       "      <td>0</td>\n",
       "      <td>Visualization of the gall bladder on F-18 FDOP...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27144</th>\n",
       "      <td>27144</td>\n",
       "      <td>0</td>\n",
       "      <td>Multidetector CT findings and differential dia...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27145 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  judgement                                               text  \\\n",
       "0          0          0  One-year age changes in MRI brain volumes in o...   \n",
       "1          1          0  Supportive CSF biomarker evidence to enhance t...   \n",
       "2          2          0  Occurrence of basal ganglia germ cell tumors w...   \n",
       "3          3          0  New developments in diagnosis and therapy of C...   \n",
       "4          4          0  Prolonged shedding of SARS-CoV-2 in an elderly...   \n",
       "...      ...        ...                                                ...   \n",
       "27140  27140          0  The amyloidogenic pathway of amyloid precursor...   \n",
       "27141  27141          0  Technologic developments in radiotherapy and s...   \n",
       "27142  27142          0  Novel screening cascade identifies MKK4 as key...   \n",
       "27143  27143          0  Visualization of the gall bladder on F-18 FDOP...   \n",
       "27144  27144          0  Multidetector CT findings and differential dia...   \n",
       "\n",
       "       fold  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         2  \n",
       "4         1  \n",
       "...     ...  \n",
       "27140     4  \n",
       "27141     1  \n",
       "27142     0  \n",
       "27143     2  \n",
       "27144     4  \n",
       "\n",
       "[27145 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9690"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"text\"].map(lambda x: len(x)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12934"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"text\"].map(lambda x: len(x)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_idx = train[train[\"fold\"] != 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, df, model_name, include_labels=True):\n",
    "        #引入BertTokenizer.from_pretrained类\n",
    "        #详情参照https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.batch_encode_plus\n",
    "        tokenizer = T.BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        self.df = df\n",
    "        self.include_labels = include_labels\n",
    "\n",
    "        self.text = df[\"text\"].tolist()\n",
    "        #使用batch_encode_plus对多个句子编码\n",
    "        #编码包括：\n",
    "        #Tokenize sentences\n",
    "        #Prepend the [CLS] token to the start \n",
    "        #Append the [SEP] token to the end\n",
    "        #Map tokens to their IDs\n",
    "        #Make IDs same max_length  \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',            \n",
    "            max_length = 256,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "        if self.include_labels:\n",
    "            self.labels = df[\"judgement\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #为啥这里要加[idx]?\n",
    "        #表示标记id表和注意力掩码的索引\n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][idx])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][idx])\n",
    "\n",
    "        if self.include_labels:\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "            return input_ids, attention_mask, label\n",
    "\n",
    "        return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = T.BertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        out = self.sigmoid(out.logits).squeeze()\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#辅助函数\n",
    "class AverageMeter(object):\n",
    "    #计算平均值用\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    #将秒转化为分\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    #持续时间\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    start = end = time.time()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        y_preds = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "\n",
    "        # record loss\n",
    "        #使用AverageMeter()更新并记录误差\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0 or step == (len(train_loader) - 1):\n",
    "            print(\n",
    "                f\"Epoch: [{epoch + 1}][{step}/{len(train_loader)}] \"\n",
    "                f\"Elapsed {timeSince(start, float(step + 1) / len(train_loader)):s} \"\n",
    "                f\"Loss: {losses.avg:.4f} \"\n",
    "            )\n",
    "    \n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    start = end = time.time()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(valid_loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # record score\n",
    "        preds.append(y_preds.to(\"cpu\").numpy())\n",
    "\n",
    "        if step % 100 == 0 or step == (len(valid_loader) - 1):\n",
    "            print(\n",
    "                f\"EVAL: [{step}/{len(valid_loader)}] \"\n",
    "                f\"Elapsed {timeSince(start, float(step + 1) / len(valid_loader)):s} \"\n",
    "                f\"Loss: {losses.avg:.4f} \"\n",
    "            )\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataloader_batchsize = 28\n",
    "\n",
    "#对测试集进行推理，获得预测结果\n",
    "def inference():\n",
    "    predictions = []\n",
    "\n",
    "    test_dataset = BaseDataset(test, \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\", include_labels=False)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=inference_dataloader_batchsize,\n",
    "        shuffle=False, num_workers=0, pin_memory=True,\n",
    "    )\n",
    "\n",
    "    for fold in range(k_fold):\n",
    "        LOGGER.info(f\"========== model: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext fold: {fold} inference ==========\")\n",
    "        model = BaseModel(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "        model.to(device)\n",
    "        model.load_state_dict(torch.load(OUTPUT_DIR + f\"PubMedBERT_base_uncased_fold{fold}_best.pth\")[\"model\"])\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        for i, (input_ids, attention_mask) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(input_ids, attention_mask)\n",
    "            preds.append(y_preds.to(\"cpu\").numpy())\n",
    "        preds = np.concatenate(preds)\n",
    "        predictions.append(preds)\n",
    "    predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloop_dataloader_batchsize = 28\n",
    "\n",
    "#将经过get_train_data()函数处理的train放入到train_loop()中\n",
    "def train_loop(train, fold):\n",
    "\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "    # ====================================================\n",
    "    # Data Loader\n",
    "    # ====================================================\n",
    "    #train[\"fold\"]不是第fold的行，其index记为trn_idx；\n",
    "    #反之其index记为val_index\n",
    "    trn_idx = train[train[\"fold\"] != fold].index\n",
    "    val_idx = train[train[\"fold\"] == fold].index\n",
    "    \n",
    "    #对训练集验证集重新reset索引\n",
    "    train_folds = train.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = train.loc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    #使用BasedataSet函数对数据集进行编码\n",
    "    train_dataset = BaseDataset(train_folds, \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "    valid_dataset = BaseDataset(valid_folds, \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=trainloop_dataloader_batchsize,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=trainloop_dataloader_batchsize,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # Model\n",
    "    # ====================================================\n",
    "    model = BaseModel(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = T.AdamW(model.parameters(),\n",
    "                        lr=2e-5,)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # ====================================================\n",
    "    # Loop\n",
    "    # ====================================================\n",
    "    best_score = -1\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(3):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        #train_fn函数返回训练集平均误差\n",
    "        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, device)\n",
    "\n",
    "        # eval\n",
    "        #valid_fn函数返回验证集平均误差和预测结果\n",
    "        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
    "        valid_labels = valid_folds[\"judgement\"].values\n",
    "\n",
    "        # scoring\n",
    "        score = fbeta_score(valid_labels, np.where(preds < border, 0, 1), beta=7.0)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        LOGGER.info(\n",
    "            f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\"\n",
    "        )\n",
    "        LOGGER.info(f\"Epoch {epoch+1} - Score: {score}\")\n",
    "        \n",
    "        #如果验证集的得分比最佳得分好，就取代最佳得分，\n",
    "        #并将模型参数保存下来\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f\"Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model\")\n",
    "            torch.save(\n",
    "                {\"model\": model.state_dict(), \"preds\": preds}, OUTPUT_DIR + f\"PubMedBERT_base_uncased_fold{fold}_best.pth\"\n",
    "            )\n",
    "    \n",
    "    #读取最佳模型参数\n",
    "    check_point = torch.load(OUTPUT_DIR + f\"PubMedBERT_base_uncased_fold{fold}_best.pth\")\n",
    "\n",
    "    valid_folds[\"preds\"] = check_point[\"preds\"]\n",
    "\n",
    "    #返回包含最佳预测结果的验证集\n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算F7score得分\n",
    "def get_result(result_df):\n",
    "    preds = result_df[\"preds\"].values\n",
    "    labels = result_df[\"judgement\"].values\n",
    "    score = fbeta_score(labels, np.where(preds < border, 0, 1), beta=7.0)\n",
    "    LOGGER.info(f\"Score: {score:<.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/775] Elapsed 0m 1s (remain 14m 54s) Loss: 0.6198 \n",
      "Epoch: [1][100/775] Elapsed 0m 53s (remain 5m 55s) Loss: 0.1419 \n",
      "Epoch: [1][200/775] Elapsed 1m 45s (remain 5m 1s) Loss: 0.1226 \n",
      "Epoch: [1][300/775] Elapsed 2m 38s (remain 4m 8s) Loss: 0.1034 \n",
      "Epoch: [1][400/775] Elapsed 3m 30s (remain 3m 16s) Loss: 0.0942 \n",
      "Epoch: [1][500/775] Elapsed 4m 23s (remain 2m 23s) Loss: 0.0873 \n",
      "Epoch: [1][600/775] Elapsed 5m 15s (remain 1m 31s) Loss: 0.0815 \n",
      "Epoch: [1][700/775] Elapsed 6m 8s (remain 0m 38s) Loss: 0.0768 \n",
      "Epoch: [1][774/775] Elapsed 6m 47s (remain 0m 0s) Loss: 0.0742 \n",
      "EVAL: [0/194] Elapsed 0m 0s (remain 0m 37s) Loss: 0.0093 \n",
      "EVAL: [100/194] Elapsed 0m 17s (remain 0m 16s) Loss: 0.0386 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0742  avg_val_loss: 0.0387  time: 442s\n",
      "Epoch 1 - Score: 0.9049102507046434\n",
      "Epoch 1 - Save Best Score: 0.9049 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [193/194] Elapsed 0m 34s (remain 0m 0s) Loss: 0.0387 \n",
      "Epoch: [2][0/775] Elapsed 0m 0s (remain 6m 53s) Loss: 0.0476 \n",
      "Epoch: [2][100/775] Elapsed 0m 53s (remain 5m 56s) Loss: 0.0382 \n",
      "Epoch: [2][200/775] Elapsed 1m 47s (remain 5m 6s) Loss: 0.0369 \n",
      "Epoch: [2][300/775] Elapsed 2m 40s (remain 4m 12s) Loss: 0.0396 \n",
      "Epoch: [2][400/775] Elapsed 3m 32s (remain 3m 18s) Loss: 0.0380 \n",
      "Epoch: [2][500/775] Elapsed 4m 25s (remain 2m 25s) Loss: 0.0394 \n",
      "Epoch: [2][600/775] Elapsed 5m 19s (remain 1m 32s) Loss: 0.0390 \n",
      "Epoch: [2][700/775] Elapsed 6m 12s (remain 0m 39s) Loss: 0.0393 \n",
      "Epoch: [2][774/775] Elapsed 6m 51s (remain 0m 0s) Loss: 0.0390 \n",
      "EVAL: [0/194] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0034 \n",
      "EVAL: [100/194] Elapsed 0m 17s (remain 0m 16s) Loss: 0.0361 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0390  avg_val_loss: 0.0361  time: 445s\n",
      "Epoch 2 - Score: 0.9092298288508557\n",
      "Epoch 2 - Save Best Score: 0.9092 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [193/194] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0361 \n",
      "Epoch: [3][0/775] Elapsed 0m 0s (remain 6m 35s) Loss: 0.0805 \n",
      "Epoch: [3][100/775] Elapsed 0m 54s (remain 6m 3s) Loss: 0.0312 \n",
      "Epoch: [3][200/775] Elapsed 1m 47s (remain 5m 7s) Loss: 0.0303 \n",
      "Epoch: [3][300/775] Elapsed 2m 40s (remain 4m 12s) Loss: 0.0285 \n",
      "Epoch: [3][400/775] Elapsed 3m 33s (remain 3m 18s) Loss: 0.0269 \n",
      "Epoch: [3][500/775] Elapsed 4m 25s (remain 2m 25s) Loss: 0.0268 \n",
      "Epoch: [3][600/775] Elapsed 5m 18s (remain 1m 32s) Loss: 0.0258 \n",
      "Epoch: [3][700/775] Elapsed 6m 11s (remain 0m 39s) Loss: 0.0266 \n",
      "Epoch: [3][774/775] Elapsed 6m 50s (remain 0m 0s) Loss: 0.0258 \n",
      "EVAL: [0/194] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0008 \n",
      "EVAL: [100/194] Elapsed 0m 17s (remain 0m 16s) Loss: 0.0374 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0258  avg_val_loss: 0.0378  time: 444s\n",
      "Epoch 3 - Score: 0.87870704534756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [193/194] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0378 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 result ==========\n",
      "Score: 0.90923\n",
      "========== fold: 1 training ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/775] Elapsed 0m 0s (remain 6m 32s) Loss: 0.6035 \n",
      "Epoch: [1][100/775] Elapsed 0m 52s (remain 5m 52s) Loss: 0.1289 \n",
      "Epoch: [1][200/775] Elapsed 1m 45s (remain 5m 1s) Loss: 0.1047 \n",
      "Epoch: [1][300/775] Elapsed 2m 37s (remain 4m 8s) Loss: 0.0908 \n",
      "Epoch: [1][400/775] Elapsed 3m 30s (remain 3m 16s) Loss: 0.0860 \n",
      "Epoch: [1][500/775] Elapsed 4m 23s (remain 2m 23s) Loss: 0.0799 \n",
      "Epoch: [1][600/775] Elapsed 5m 15s (remain 1m 31s) Loss: 0.0752 \n",
      "Epoch: [1][700/775] Elapsed 6m 8s (remain 0m 38s) Loss: 0.0727 \n",
      "Epoch: [1][774/775] Elapsed 6m 47s (remain 0m 0s) Loss: 0.0701 \n",
      "EVAL: [0/194] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0476 \n",
      "EVAL: [100/194] Elapsed 0m 17s (remain 0m 16s) Loss: 0.0410 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0701  avg_val_loss: 0.0410  time: 441s\n",
      "Epoch 1 - Score: 0.8969607116382506\n",
      "Epoch 1 - Save Best Score: 0.8970 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [193/194] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0410 \n",
      "Epoch: [2][0/775] Elapsed 0m 0s (remain 6m 34s) Loss: 0.0177 \n",
      "Epoch: [2][100/775] Elapsed 0m 53s (remain 5m 54s) Loss: 0.0316 \n",
      "Epoch: [2][200/775] Elapsed 1m 45s (remain 5m 2s) Loss: 0.0364 \n",
      "Epoch: [2][300/775] Elapsed 2m 38s (remain 4m 9s) Loss: 0.0344 \n",
      "Epoch: [2][400/775] Elapsed 3m 31s (remain 3m 17s) Loss: 0.0348 \n",
      "Epoch: [2][500/775] Elapsed 4m 23s (remain 2m 24s) Loss: 0.0345 \n",
      "Epoch: [2][600/775] Elapsed 5m 16s (remain 1m 31s) Loss: 0.0340 \n",
      "Epoch: [2][700/775] Elapsed 6m 9s (remain 0m 38s) Loss: 0.0343 \n",
      "Epoch: [2][774/775] Elapsed 6m 48s (remain 0m 0s) Loss: 0.0344 \n",
      "EVAL: [0/194] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0370 \n",
      "EVAL: [100/194] Elapsed 0m 17s (remain 0m 16s) Loss: 0.0367 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0344  avg_val_loss: 0.0380  time: 442s\n",
      "Epoch 2 - Score: 0.9187547456340166\n",
      "Epoch 2 - Save Best Score: 0.9188 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [193/194] Elapsed 0m 34s (remain 0m 0s) Loss: 0.0380 \n",
      "Epoch: [3][0/775] Elapsed 0m 0s (remain 6m 35s) Loss: 0.0031 \n",
      "Epoch: [3][100/775] Elapsed 0m 53s (remain 5m 55s) Loss: 0.0220 \n",
      "Epoch: [3][200/775] Elapsed 1m 46s (remain 5m 2s) Loss: 0.0216 \n",
      "Epoch: [3][300/775] Elapsed 2m 39s (remain 4m 11s) Loss: 0.0215 \n",
      "Epoch: [3][400/775] Elapsed 3m 32s (remain 3m 17s) Loss: 0.0190 \n",
      "Epoch: [3][500/775] Elapsed 4m 25s (remain 2m 24s) Loss: 0.0190 \n",
      "Epoch: [3][600/775] Elapsed 5m 18s (remain 1m 32s) Loss: 0.0201 \n",
      "Epoch: [3][700/775] Elapsed 6m 12s (remain 0m 39s) Loss: 0.0212 \n",
      "Epoch: [3][774/775] Elapsed 6m 52s (remain 0m 0s) Loss: 0.0213 \n",
      "EVAL: [0/194] Elapsed 0m 0s (remain 0m 37s) Loss: 0.0368 \n",
      "EVAL: [100/194] Elapsed 0m 17s (remain 0m 16s) Loss: 0.0381 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0213  avg_val_loss: 0.0386  time: 447s\n",
      "Epoch 3 - Score: 0.8759580791490692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [193/194] Elapsed 0m 34s (remain 0m 0s) Loss: 0.0386 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 1 result ==========\n",
      "Score: 0.91875\n",
      "========== fold: 2 training ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/775] Elapsed 0m 0s (remain 6m 33s) Loss: 0.7494 \n",
      "Epoch: [1][100/775] Elapsed 0m 53s (remain 5m 56s) Loss: 0.1303 \n",
      "Epoch: [1][200/775] Elapsed 1m 46s (remain 5m 4s) Loss: 0.1074 \n",
      "Epoch: [1][300/775] Elapsed 2m 39s (remain 4m 11s) Loss: 0.0896 \n",
      "Epoch: [1][400/775] Elapsed 3m 32s (remain 3m 18s) Loss: 0.0848 \n",
      "Epoch: [1][500/775] Elapsed 4m 25s (remain 2m 25s) Loss: 0.0787 \n",
      "Epoch: [1][600/775] Elapsed 5m 18s (remain 1m 32s) Loss: 0.0738 \n",
      "Epoch: [1][700/775] Elapsed 6m 11s (remain 0m 39s) Loss: 0.0702 \n",
      "Epoch: [1][774/775] Elapsed 6m 51s (remain 0m 0s) Loss: 0.0696 \n",
      "EVAL: [0/194] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0195 \n",
      "EVAL: [100/194] Elapsed 0m 17s (remain 0m 16s) Loss: 0.0428 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0696  avg_val_loss: 0.0453  time: 445s\n",
      "Epoch 1 - Score: 0.8773171076835998\n",
      "Epoch 1 - Save Best Score: 0.8773 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [193/194] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0453 \n",
      "Epoch: [2][0/775] Elapsed 0m 0s (remain 6m 37s) Loss: 0.0137 \n",
      "Epoch: [2][100/775] Elapsed 0m 53s (remain 5m 57s) Loss: 0.0313 \n",
      "Epoch: [2][200/775] Elapsed 1m 46s (remain 5m 4s) Loss: 0.0406 \n",
      "Epoch: [2][300/775] Elapsed 2m 39s (remain 4m 11s) Loss: 0.0389 \n",
      "Epoch: [2][400/775] Elapsed 3m 32s (remain 3m 18s) Loss: 0.0370 \n",
      "Epoch: [2][500/775] Elapsed 4m 25s (remain 2m 25s) Loss: 0.0344 \n",
      "Epoch: [2][600/775] Elapsed 5m 18s (remain 1m 32s) Loss: 0.0359 \n",
      "Epoch: [2][700/775] Elapsed 6m 11s (remain 0m 39s) Loss: 0.0353 \n",
      "Epoch: [2][774/775] Elapsed 6m 51s (remain 0m 0s) Loss: 0.0356 \n",
      "EVAL: [0/194] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0037 \n",
      "EVAL: [100/194] Elapsed 0m 17s (remain 0m 16s) Loss: 0.0330 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0356  avg_val_loss: 0.0330  time: 445s\n",
      "Epoch 2 - Score: 0.9152798789712556\n",
      "Epoch 2 - Save Best Score: 0.9153 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [193/194] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0330 \n",
      "Epoch: [3][0/775] Elapsed 0m 0s (remain 6m 38s) Loss: 0.0199 \n",
      "Epoch: [3][100/775] Elapsed 0m 53s (remain 5m 57s) Loss: 0.0197 \n",
      "Epoch: [3][200/775] Elapsed 1m 46s (remain 5m 4s) Loss: 0.0202 \n",
      "Epoch: [3][300/775] Elapsed 2m 39s (remain 4m 11s) Loss: 0.0210 \n",
      "Epoch: [3][400/775] Elapsed 3m 32s (remain 3m 18s) Loss: 0.0194 \n",
      "Epoch: [3][500/775] Elapsed 4m 25s (remain 2m 25s) Loss: 0.0192 \n",
      "Epoch: [3][600/775] Elapsed 5m 19s (remain 1m 32s) Loss: 0.0197 \n",
      "Epoch: [3][700/775] Elapsed 6m 12s (remain 0m 39s) Loss: 0.0210 \n",
      "Epoch: [3][774/775] Elapsed 6m 51s (remain 0m 0s) Loss: 0.0206 \n",
      "EVAL: [0/194] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0086 \n",
      "EVAL: [100/194] Elapsed 0m 17s (remain 0m 16s) Loss: 0.0533 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0206  avg_val_loss: 0.0523  time: 445s\n",
      "Epoch 3 - Score: 0.8537721204594847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [193/194] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0523 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 2 result ==========\n",
      "Score: 0.91528\n",
      "========== fold: 3 training ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/775] Elapsed 0m 0s (remain 6m 33s) Loss: 0.7937 \n",
      "Epoch: [1][100/775] Elapsed 0m 53s (remain 5m 54s) Loss: 0.1328 \n",
      "Epoch: [1][200/775] Elapsed 1m 46s (remain 5m 3s) Loss: 0.1101 \n",
      "Epoch: [1][300/775] Elapsed 2m 39s (remain 4m 10s) Loss: 0.0987 \n",
      "Epoch: [1][400/775] Elapsed 3m 32s (remain 3m 17s) Loss: 0.0897 \n",
      "Epoch: [1][500/775] Elapsed 4m 25s (remain 2m 24s) Loss: 0.0822 \n",
      "Epoch: [1][600/775] Elapsed 5m 17s (remain 1m 32s) Loss: 0.0761 \n",
      "Epoch: [1][700/775] Elapsed 6m 10s (remain 0m 39s) Loss: 0.0731 \n",
      "Epoch: [1][774/775] Elapsed 6m 50s (remain 0m 0s) Loss: 0.0700 \n",
      "EVAL: [0/194] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0317 \n",
      "EVAL: [100/194] Elapsed 0m 17s (remain 0m 16s) Loss: 0.0432 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0700  avg_val_loss: 0.0445  time: 444s\n",
      "Epoch 1 - Score: 0.9146341463414636\n",
      "Epoch 1 - Save Best Score: 0.9146 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [193/194] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0445 \n",
      "Epoch: [2][0/775] Elapsed 0m 0s (remain 6m 36s) Loss: 0.0220 \n",
      "Epoch: [2][100/775] Elapsed 0m 55s (remain 6m 7s) Loss: 0.0446 \n",
      "Epoch: [2][200/775] Elapsed 1m 48s (remain 5m 10s) Loss: 0.0358 \n",
      "Epoch: [2][300/775] Elapsed 2m 41s (remain 4m 14s) Loss: 0.0334 \n",
      "Epoch: [2][400/775] Elapsed 3m 35s (remain 3m 20s) Loss: 0.0322 \n",
      "Epoch: [2][500/775] Elapsed 4m 28s (remain 2m 27s) Loss: 0.0325 \n",
      "Epoch: [2][600/775] Elapsed 5m 22s (remain 1m 33s) Loss: 0.0337 \n",
      "Epoch: [2][700/775] Elapsed 6m 15s (remain 0m 39s) Loss: 0.0325 \n",
      "Epoch: [2][774/775] Elapsed 6m 55s (remain 0m 0s) Loss: 0.0328 \n",
      "EVAL: [0/194] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0051 \n",
      "EVAL: [100/194] Elapsed 0m 17s (remain 0m 16s) Loss: 0.0402 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0328  avg_val_loss: 0.0397  time: 449s\n",
      "Epoch 2 - Score: 0.8639308855291578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [193/194] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0397 \n",
      "Epoch: [3][0/775] Elapsed 0m 0s (remain 6m 41s) Loss: 0.0006 \n",
      "Epoch: [3][100/775] Elapsed 0m 53s (remain 5m 59s) Loss: 0.0176 \n",
      "Epoch: [3][200/775] Elapsed 1m 47s (remain 5m 6s) Loss: 0.0187 \n",
      "Epoch: [3][300/775] Elapsed 2m 40s (remain 4m 12s) Loss: 0.0213 \n",
      "Epoch: [3][400/775] Elapsed 3m 33s (remain 3m 19s) Loss: 0.0206 \n",
      "Epoch: [3][500/775] Elapsed 4m 27s (remain 2m 26s) Loss: 0.0213 \n",
      "Epoch: [3][600/775] Elapsed 5m 20s (remain 1m 32s) Loss: 0.0212 \n",
      "Epoch: [3][700/775] Elapsed 6m 13s (remain 0m 39s) Loss: 0.0215 \n",
      "Epoch: [3][774/775] Elapsed 6m 53s (remain 0m 0s) Loss: 0.0216 \n",
      "EVAL: [0/194] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0122 \n",
      "EVAL: [100/194] Elapsed 0m 17s (remain 0m 16s) Loss: 0.0291 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0216  avg_val_loss: 0.0364  time: 447s\n",
      "Epoch 3 - Score: 0.8653408228328526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [193/194] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0364 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 result ==========\n",
      "Score: 0.91463\n",
      "========== fold: 4 training ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/775] Elapsed 0m 0s (remain 6m 32s) Loss: 0.6470 \n",
      "Epoch: [1][100/775] Elapsed 0m 53s (remain 5m 55s) Loss: 0.1291 \n",
      "Epoch: [1][200/775] Elapsed 1m 46s (remain 5m 3s) Loss: 0.1103 \n",
      "Epoch: [1][300/775] Elapsed 2m 39s (remain 4m 10s) Loss: 0.0959 \n",
      "Epoch: [1][400/775] Elapsed 3m 32s (remain 3m 17s) Loss: 0.0842 \n",
      "Epoch: [1][500/775] Elapsed 4m 25s (remain 2m 24s) Loss: 0.0773 \n",
      "Epoch: [1][600/775] Elapsed 5m 18s (remain 1m 32s) Loss: 0.0731 \n",
      "Epoch: [1][700/775] Elapsed 6m 11s (remain 0m 39s) Loss: 0.0684 \n",
      "Epoch: [1][774/775] Elapsed 6m 50s (remain 0m 0s) Loss: 0.0660 \n",
      "EVAL: [0/194] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0060 \n",
      "EVAL: [100/194] Elapsed 0m 17s (remain 0m 16s) Loss: 0.0507 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0660  avg_val_loss: 0.0532  time: 444s\n",
      "Epoch 1 - Score: 0.880540064572938\n",
      "Epoch 1 - Save Best Score: 0.8805 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [193/194] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0532 \n",
      "Epoch: [2][0/775] Elapsed 0m 0s (remain 6m 38s) Loss: 0.0091 \n",
      "Epoch: [2][100/775] Elapsed 0m 53s (remain 5m 56s) Loss: 0.0368 \n",
      "Epoch: [2][200/775] Elapsed 1m 46s (remain 5m 3s) Loss: 0.0340 \n",
      "Epoch: [2][300/775] Elapsed 2m 39s (remain 4m 10s) Loss: 0.0317 \n",
      "Epoch: [2][400/775] Elapsed 3m 32s (remain 3m 18s) Loss: 0.0337 \n",
      "Epoch: [2][500/775] Elapsed 4m 25s (remain 2m 25s) Loss: 0.0328 \n",
      "Epoch: [2][600/775] Elapsed 5m 18s (remain 1m 32s) Loss: 0.0325 \n",
      "Epoch: [2][700/775] Elapsed 6m 11s (remain 0m 39s) Loss: 0.0319 \n",
      "Epoch: [2][774/775] Elapsed 6m 50s (remain 0m 0s) Loss: 0.0318 \n",
      "EVAL: [0/194] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0004 \n",
      "EVAL: [100/194] Elapsed 0m 17s (remain 0m 16s) Loss: 0.0602 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0318  avg_val_loss: 0.0683  time: 444s\n",
      "Epoch 2 - Score: 0.6662486283116477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [193/194] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0683 \n",
      "Epoch: [3][0/775] Elapsed 0m 0s (remain 6m 39s) Loss: 0.0006 \n",
      "Epoch: [3][100/775] Elapsed 0m 53s (remain 5m 58s) Loss: 0.0242 \n",
      "Epoch: [3][200/775] Elapsed 1m 46s (remain 5m 5s) Loss: 0.0227 \n",
      "Epoch: [3][300/775] Elapsed 2m 40s (remain 4m 12s) Loss: 0.0202 \n",
      "Epoch: [3][400/775] Elapsed 3m 33s (remain 3m 18s) Loss: 0.0206 \n",
      "Epoch: [3][500/775] Elapsed 4m 26s (remain 2m 25s) Loss: 0.0219 \n",
      "Epoch: [3][600/775] Elapsed 5m 19s (remain 1m 32s) Loss: 0.0211 \n",
      "Epoch: [3][700/775] Elapsed 6m 12s (remain 0m 39s) Loss: 0.0215 \n",
      "Epoch: [3][774/775] Elapsed 6m 52s (remain 0m 0s) Loss: 0.0212 \n",
      "EVAL: [0/194] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0006 \n",
      "EVAL: [100/194] Elapsed 0m 17s (remain 0m 16s) Loss: 0.0412 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0212  avg_val_loss: 0.0457  time: 446s\n",
      "Epoch 3 - Score: 0.8564814814814816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [193/194] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0457 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 4 result ==========\n",
      "Score: 0.88054\n",
      "========== CV ==========\n",
      "Score: 0.90757\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "oof_df = pd.DataFrame()\n",
    "for fold in range(k_fold):\n",
    "    _oof_df = train_loop(train, fold)\n",
    "    oof_df = pd.concat([oof_df, _oof_df])\n",
    "    LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "    #第k折的得分\n",
    "    get_result(_oof_df)\n",
    "\n",
    "# CV result\n",
    "#k折平均得分\n",
    "LOGGER.info(f\"========== CV ==========\")\n",
    "get_result(oof_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save OOF result\n",
    "# 存储包含最佳预测结果的验证集\n",
    "oof_df.to_csv(OUTPUT_DIR + \"oof_df_pubmed_12.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== model: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext fold: 0 inference ==========\n",
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b8c9e7a66d4110bd0ba38ae1d17bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1459.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== model: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext fold: 1 inference ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ec7cf06cf9406b996daf7e96f25797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1459.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== model: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext fold: 2 inference ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf98f89ff3a946d094ffe6f4e498d2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1459.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== model: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext fold: 3 inference ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6525523c04d449c793c6592c5ad474b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1459.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== model: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext fold: 4 inference ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dcd2beab31a4427b6561c9f26f1006d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1459.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "predictions = inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "borders = []\n",
    "list = range(10,90,1)\n",
    "for x in list:\n",
    "    x = x/1000\n",
    "    borders.append(x)\n",
    "\n",
    "\n",
    "for border in borders:\n",
    "    preds = np.where(predictions < border, 0, 1)\n",
    "    # submission\n",
    "    sub[\"judgement\"] = preds\n",
    "    n = str(border)\n",
    "    sub.to_csv(OUTPUT_DIR + \"bert\"+n+\".csv\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
